DP、TP、PP、SP、EP 五种不同的并行技术

# 一、DP（Data Parallelism）- 数据并行
这是最基础、最通用的并行方式
- **切分对象：数据（Bath Size）**。
- **如何工作**：
    - 假设模型有 10GB，你有4张卡。
    - **复制**：每张卡上都完整复制一份这 10GB 的模型。
    - **分发**：假如这轮要训练 8 个数据样本，每张卡领 2 个样本回去算。
    - **同步**：算完之后，大家把计算出的梯度（Gradient）汇总，求平均，然后更新各自的模型，保证大家步调一致。
- **现代进化版（ZeRO/FSDP）**：传统的 DP 每张卡都要存完整模型，太浪费显存。现在的 DP（如DeepSpeed ZeRO，PyTorch FSDP）会把**模型参数、梯度、优化器状态**切碎了存在不同的卡上，用的时候再临时拉过来。这叫“显存切片”。
- **优缺点**：
    - **优点**：实现简单，通信频率适中，扩展性好。
    - **缺点**：如果单张卡的显存连一个模型切片都放不下，就跑不起来。
- **通信特点**：主要发生在反向传播后，传输梯度。

## 1.1 FSDP
**FSDP的核心思想是：**把模型参数、梯度、优化器状态**切碎**，均匀地分配到所有GPU上。
- 如果有 8 张卡，每张卡只存 1/8 的参数。
- 这样，8 张 80GB 的卡加起来就像一张 640GB 的超级大卡。

### 1.1.1 核心原理（ZeRO-3）
FSDP的原理源自微软 DeepSpeed 的ZeRO论文。它分为三个阶段，FSDP默认对应ZeRO-3：

**A. 切分什么**

大模型训练中显存主要被三样东西占用，FSDP把它们全部切分：
- **Optimizer States（优化器状态）**：例如 Adam 的 momentum 和 variance（占比最大）。
- **Gradients（梯度）**：反向传播算出来的。
- **Parameters（模型参数）**：模型权重本身。

**B. 怎么计算？（ALL-Gather与Reduce-Scatter）**

既然每张卡只有 1/N 的参数，那怎么做前向计算（需要完整权重）呢？

答案是：**现用现拿，用完即扔**。

假设我们计算 Transformer 的第 3 层：
- **前向传播（Forward）**：
    - **All-Gather**：所有GPU广播自己持有的“第 3 层碎片”。
    - 瞬间，每张GPU上都凑齐了**完整的第 3 层参数**。
    - Compute：计算第 3 层的输出。
    - Discard：立即删除刚刚凑齐的第 3 层完整参数，只保留原本属于自己的那 1/N 碎片。显存瞬间释放。
    - 继续处理第 4 层...
- **反向传播（Backward）**：
    - **All-Gather**：再次广播凑齐第 3 层完整参数。
    - **Compute**：计算第 3 层梯度。
    - **Discard**：再次删除完整参数。
    - **Reduce-Scatter**：计算出的梯度时完整的，但我只需要维护我负责的那 1/N。大家交换梯度，把对应部分的梯度加和后发给负责人，其余丢弃。

**C. 代价是什么？**

    **用“通信带宽”换“显存空间”**

    FSDP 极大地降低了显存占用，但显著增加了 GPU 之间的通信量（频繁的ALL-Gather）。因此，FSDP 极其依赖高速网络（如 NVLink或InfiniBand）。

### 1.1.2 Pytorch FSDP代码实现
[代码直接查看这里](./fsdp.py)

# 二、TP（Tensor Parallelism）-张量并行
当模型大到连单个层（Layer）的矩阵都塞不进一张显卡时，就需要 TP。

- **切分对象：模型内部的矩阵**。
- **如何工作**：
    - 模型是由很多矩阵乘法组成的 $(Y=X\times W)$ 。
    - TP 是把巨大的权重矩阵 $W$ **竖着切或横着切**。
- **优缺点**：
    - **优点**：减少了单卡的显存压力，不增加模型的层数深度。
    - **缺点**：**通信量巨大**，因为每算完一层的一个矩阵乘法，所有GPU就需要交换一次数据（ALL-Reduce）。
- **性能建议**：只能在**单机内部**使用，必须利用NVLink这种超高速互联，绝对不能跨机器使用。

# 三、PP（Pipeline Parallelism）-流水线并行
当模型层数非常深（比如GPT-3有 96 层），我们可以把它“横着切”。
- **切分对象：模型的层（Layers）**。
- **如何工作**：
    - 假设模型有40层，你有4张卡。
    - GPU 1负责第 1-10 层；GPU 2 负责 11-20 层....
- **优缺点**：
    - **优点**：通信量极小（只在层与层交界处传输），适合**跨机器**并行。
    - **缺点**：**气泡（Bubble）问题**。当GPU 4在干活时，GPU 1 可能在等下一批数据，导致会有设备空闲。
- **性能建议**：通常 TP、DP结合使用（所谓的 3D 并行）。

# 四、SP（Sequence Parallelism）-序列并行
这是为了解决**长文本（Long Context）**训练而成的技术（比如Kimi、GPT-4-128k）。
- **切分对象：序列长度**。
- **如何工作**：
    - 假设一句话有 10 万个字（Token）。这导致中间的 Attention 计算量和显存爆炸。
    - SP 把这 10 万个字切成几段，每张卡负责算其中一段的Attention（通常结合Ring Attention等技术）。
- **优缺点**：
    - **优点**：打破了单卡显存对序列长度的限值，是训练超长上下文模型的必备技术。
    - **缺点**：实现极其复杂，需要对Attention机制进行底层修改。

# 五、EP（Expert Parallelism）-专家并行
这是专门针对 MoE（混合专家）模型的技术。
- **切分对象：专家网络**。
- **如何工作**：
    - MeE 模型里有很多“专家”（小的神经网络）。
    - EP 把不同的专家放在不同的GPU上。
    - 当一个Token进来，如果路由网关（Router）认为它属于“数学专家”，就把它发给存有数学专家的GPU去处理。
- **优缺点**：
    - **优点**：极大扩展模型参数量（可以做到万亿参数），但推理计算量保持较低。
    - **缺点**：**负载均衡难**（可能大家都去访问热门专家，冷门专家闲着）；通信模式是复杂的“All-to-All”。