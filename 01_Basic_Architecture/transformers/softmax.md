# Softmax
## 一、Softmax的作用
Softmax函数通常用于神经网络的最后一层（主要是做分类任务）或者中间层的Attention中，其核心就是将一个任意的实数向量（通常称之为Logits）“归一化”为一个**概率分布**的向量。

具体说来就是，我们假设输入的向量为 $z=[z_1,z_2,\dots,z_n]$ ，Softmax 输出第 $i$ 个元素的公式为：

$$
\sigma (z)_i=\frac{e^{z_i}}{\sum^{n}_{j=1}e^{z_j}}
$$

**主要作用体现为**：
- **归一化(Normalization)**：无论输入的 $z_i$ 是正数、负数还是零，经过Softmax后，所有输出值都在 $(0,1)$ 之间。
- **概率分布**：所有输出元素的**和**严格等于 $1(\sum{\sigma(z)_i}=1)$ 。这使得输出可以被解释为“概率”。
- **可导性**：它是平滑的，处处可导，非常适合用于梯度下降反向传播。
    - 平滑主要体现在，我们假设对于 $[0.2,2.1]$ Softmax输出是： $A=0.475,B=0.525$ 。如果我们让A增加一点点变成2.11.Softmax的输出就会变成： $A=0.502,B=0.498$ 。**输入的微小变化，对应输出的微小变化**。这种连续、渐进的变化，就是“平滑”。
    - softmax因为是 $e^x$ ，是数学界性质最好的函数之一，它的导数就是它自己。

## 二、为什么使用幂指数 $(e^x)$ ？
为什么不用绝对值、平方或者其他的函数，偏偏使用 $e$ 的幂次方？主要有以下几个原因：
- **非负性**：概率不能是负数。 $e^x$ 的结果永远是正数，这就把输入的负数映射到了正数区间，保证了分子分母皆为正。
- **拉大差异**：指数函数的增长速度非常快。它会通过指数放大原本数值较大的元素，抑制数值较小的元素。这使得模型在做分类或注意力选择时，决策更加果断。
- **求导极其方便**： $e^x$ 的导数就是本身。这在反向传播计算梯度时，Softmax结合交叉熵损失函数，其梯度的数学形式非常简洁，计算效率高且数值稳定。推导如下：
    - **输入（Logits）**： $z=[z_1,z_2,\dots,z_n]$ 
    - **Softmax输出（概率预测）**： $a_i=\frac{e^{z_i}}{\sum_{k}e^{z_k}}$
    - **真实标签（One-hot）**： $y=[0,\dots,1,\dots,0]$ 假设第 $t$ 类是真值，即 $\sum_{k}=1$
    - **交叉熵损失**： $L=-\sum_{k}ln(a_k)$ 。
    - 我们的目标是求：损失函数 $L$ 对输入 $z_i$ 的梯度 $\frac{\partial L}{\partial z_i}$ 。
        - 第一步：Loss对 $a$ 求导：很简单，因为 $L=-\sum_{k}ln(a_k)$ ， $ln(x)$ 的导数是 $1/x$ 。 $\frac{\partial L}{\partial a_j}=-\frac{y_j}{a_j}$
        - 第二步：Softmax(a) 对 $z$ 求导。这是最复杂的一步。因为Softmax的分母包含所有 $z$ ，所以改变 $z_i$ 会影响所有的 $a_j$ 。分两种情况：
            - 当 $i=j$ 时：推导后得到 $a_i(1-a_i)$
            - 当 $i\neq j$ 时：推导后得到 $-a_ja_i$
        - 第三步：合并，我们将上述两步结合起来计算 $\frac{\partial L}{\partial z_i}$ 就会得到： $\frac{\partial L}{\partial z_i}=\sum_j{\frac{\partial L}{\partial a_j}\cdot{\frac{\partial a_j}{\partial z_i}}}$ 代入第一步的结果： $=\sum_j{(-\frac{y_j}{a_j})\cdot \frac{\partial a_j}{\partial z_i}}$ 。为了利用第二步的结论，我们将求和拆分为 $j=i$ 何 $j\neq i$ 两部分： 

$$
\begin{align}
\frac{\partial L}{\partial z_i}&=-\frac{y_i}{a_i}\cdot a_i(1-a_i)+\sum_{j\neq i} (-\frac{y_j}{a_j})\cdot (-a_j a_i) \\
&=-y_i(1-a_i)+\sum_{j\neq i}(y_j a_i)\\
&=-y_i+y_i a_i+\sum_{j\neq i}y_j a_i\\
&=-y_i+a_i(1)
\end{align}
$$

这里(3)->(4)主要是因为 $y$ 是One-hot向量，所有元素的和为 $1$ ，即 $\sum y_k=1$ 。

这个结果之所以重要有以下几点原因：
- **计算高效**：在反向传播时，我们不需要重新计算复杂的指数、对数或除法导数。计算机只需要做一次减法，这极大的加快了训练速度。
- **梯度与误差成正比（解决梯度消失问题）**：这个公式告诉我们，**梯度的物理意义就是误差**。如果预测很离谱，梯度就是 $-0.9$ ，绝对值很大 $\to$ **参数更新步子大，快速修正**。如果预测很准，梯度就是 $-0.01$ ，绝对值很小 $\to$ **微调，稳定收敛**。

## 三、在Attention中按Softmax公式计算有什么问题？
Attention的核心公式是：

$$
\text{Attention}(Q,K,V)=\text{Softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$

如果直接按照数学定义 $\frac{e^{x_i}}{\sum e^{x_j}}$ 编写代码，会遇到严重的**数值不稳定性**的问题：**上溢和下溢**。
- **数值上溢**：
    - 指数函数的增长速度极快。如果 $QK^T$ 计算出的某个值比较大，在float32 精度下， $e^{88}$ 左右就已经达到上限。 $e^{1000}$ 会直接超出计算机浮点数的表示范围，导致结果变成 inf 无穷大。
    - 计算机计算 $\frac{inf}{inf}$ 会得到 NaN，导致模型训练崩溃。
- **数值下溢**：
    - 如果 $z_i$ 是非常小的负数，如 $e^{-1000}$ 会因为精度限值变成纯粹的 0 。
    - 虽然分子为 0 问题不大，但如果分母中所有项都下溢变成了 0，就会出现除以 0 的错误。

## 四、在工程实现的时候怎么做的？
为了解决数值溢出问题，工程上普遍采用“**减去最大值**”的方法。

**数学推导**

Softmax函数具有**平移不变性**。如果我们给输入向量的所有元素都减去同一个常熟 $C$ ，Softmax的结果不变：

$$
\begin{align}
\frac{e^{x_i-C}}{\sum_j{e^{x_j-C}}}=
\frac{e^{x_i}\cdot e^{-C}}{\sum_j{e^{x_j}\cdot e^{-C}}}=
\frac{e^{x_i}\cdot e^{-C}}{e^{-C}\cdot \sum_j{e^{x_j}}}=
\frac{e^{x_i}}{\sum_j{e^{x_j}}}
\end{align}
$$

**工程实现步骤**

为了防止溢出，我们通常取 $C=\text{max}(x)$ （输入向量中的最大值）。

**具体步骤如下**：
- **找到最大值**：找出输入向量 $x$ 中的最大值 $M=\text{max}(x_1,\dots,x_n)$ 。
- **平移**：计算新向量 $x^{'}=x-M$ 。
    - 此时，向量中最大的元素变成了 $0(M-M=0)$ 。
    - 其他所有元素都是负数或 $0$ 。
- **计算指数**：计算 $e^{x^{'}}$ .
    - 因为最大值是 $0$ ，所以 $e^0 = 1$ 。
    - 其他项 $e^{\text{负数}}$ 的范围在 $(0,1]$ 之间。
    - **彻底解决了上溢（Overflow）问题**，因为永远不会超过 $1$ 。
- **处理下溢**：
    - 对于极小的负数， $e^{x^{'}}$ 可能会变成 $0$ （下溢）。但这没关系，因为分母中至少有一项是 $e^0 =1$ （来自最大值那一项），所以分母永远 $\ge 1$ ，**永远不会发生除以 0 的情况**。

## 五、痛点：普通Softmax在GPU上的“内存墙”
在标准的Attention计算中（ $\text{Attention}(Q,K,V)=\text{Softmax}(\frac{QK^T}{\sqrt d})V$ ），主要步骤如下：
1. **MatMul**：计算 $S=QK^T$ 。将这个巨大的 $N\times N$ 矩阵写入**HBM（显存）**。
2. **Softmax Step 1（Max）**：从 HBM 读取 $S$ ，按行找最大值 $m$ ，写入 HBM。
3. **Softmax Step 2（Exp & Sum）**：从 HBM 读取 $S$ 和 $m$ ，计算 $e^{S-m}$ 和总和 $\ell$ ,写入 HBM。
4. **Softmax Step 3（Div）**：读取分子和分母，做除法得到 $P$ ，写入 HBM。
5. **MatMul**：读取 $P$ 和 $V$ ，计算结果 $O$ 。

**问题在于**：GPU的计算速度（FLOPs）非常快，但显存带宽（HBM Bandwidth）是瓶颈。上述过程频繁地对那个巨大的 $N\times N$ 矩阵进行读写。
- 需要把整个矩阵生成出来存到显存，再读出来做Softmax，再存回去。
- 把序列长度 $N$ 很大时，显存不仅容易爆（OOM），而且大部分时间都浪费在搬运数据上。

**解决方案**：请参考[OneLine Softmax与Tiling（FlashAttention）](../../02_Training_Optimization/flash_attention/)