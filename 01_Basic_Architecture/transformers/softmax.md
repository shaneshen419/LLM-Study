# Softmax
## 一、Softmax的作用
Softmax函数通常用于神经网络的最后一层（主要是做分类任务）或者中间层的Attention中，其核心就是将一个任意的实数向量（通常称之为Logits）“归一化”为一个**概率分布**的向量。

具体说来就是，我们假设输入的向量为 $z=[z_1,z_2,\dots,z_n]$ ，Softmax 输出第 $i$ 个元素的公式为：

$$
\sigma (z)_i=\frac{e^{z_i}}{\sum^{n}_{j=1}e^{z_j}}
$$

**主要作用体现为**：
- **归一化(Normalization)**：无论输入的 $z_i$ 是正数、负数还是零，经过Softmax后，所有输出值都在 $(0,1)$ 之间。
- **概率分布**：所有输出元素的**和**严格等于 $1(\sum{\sigma(z)_i}=1)$ 。这使得输出可以被解释为“概率”。
- **可导性**：它是平滑的，处处可导，非常适合用于梯度下降反向传播。
    - 平滑主要体现在，我们假设对于 $[0.2,2.1]$ Softmax输出是： $A=0.475,B=0.525$ 。如果我们让A增加一点点变成2.11.Softmax的输出就会变成： $A=0.502,B=0.498$ 。**输入的微小变化，对应输出的微小变化**。这种连续、渐进的变化，就是“平滑”。
    - softmax因为是 $e^x$ ，是数学界性质最好的函数之一，它的导数就是它自己。

## 二、为什么使用幂指数 $(e^x)$ ？
为什么不用绝对值、平方或者其他的函数，偏偏使用 $e$ 的幂次方？主要有以下几个原因：
- **非负性**：概率不能是负数。 $e^x$ 的结果永远是正数，这就把输入的负数映射到了正数区间，保证了分子分母皆为正。
- **拉大差异**：指数函数的增长速度非常快。它会通过指数放大原本数值较大的元素，抑制数值较小的元素。这使得模型在做分类或注意力选择时，决策更加果断。
- **求导极其方便**： $e^x$ 的导数就是本身。这在反向传播计算梯度时，Softmax结合交叉熵损失函数，其梯度的数学形式非常简洁，计算效率高且数值稳定。推导如下：
    - **输入（Logits）**： $z=[z_1,z_2,\dots,z_n]$ 
    - **Softmax输出（概率预测）**： $a_i=\frac{e^{z_i}}{\sum_{k}e^{z_k}}$
    - **真实标签（One-hot）**： $y=[0,\dots,1,\dots,0]$ 假设第 $t$ 类是真值，即 $\sum_{k}=1$
    - **交叉熵损失**： $L=-\sum_{k}ln(a_k)$ 。
    - 我们的目标是求：损失函数 $L$ 对输入 $z_i$ 的梯度 $\frac{\partial L}{\partial z_i}$ 。
        - 第一步：Loss对 $a$ 求导：很简单，因为 $L=-\sum_{k}ln(a_k)$ ， $ln(x)$ 的导数是 $1/x$ 。 $\frac{\partial L}{\partial a_j}=-\frac{y_j}{a_j}$
        - 第二步：Softmax(a) 对 $z$ 求导。这是最复杂的一步。因为Softmax的分母包含所有 $z$ ，所以改变 $z_i$ 会影响所有的 $a_j$ 。分两种情况：
            - 当 $i=j$ 时：推导后得到 $a_i(1-a_i)$
            - 当 $i\neq j$ 时：推导后得到 $-a_ja_i$
        - 第三步：合并，我们将上述两步结合起来计算 $\frac{\partial L}{\partial z_i}$ 就会得到： $\frac{\partial L}{\partial z_i}=\sum_j{\frac{\partial L}{\partial a_j}\cdot{\frac{\partial a_j}{\partial z_i}}}$ 代入第一步的结果： $=\sum_j{(-\frac{y_j}{a_j})\cdot \frac{\partial a_j}{\partial z_i}}$ 。为了利用第二步的结论，我们将求和拆分为 $j=i$ 何 $j\neq i$ 两部分： 

$$
\begin{align}
\frac{\partial L}{\partial z_i}&=-\frac{y_i}{a_i}\cdot a_i(1-a_i)+\sum_{j\neq i} (-\frac{y_j}{a_j})\cdot (-a_j a_i) \\
&=-y_i(1-a_i)+\sum_{j\neq i}(y_j a_i)\\
&=-y_i+y_i a_i+\sum_{j\neq i}y_j a_i\\
&=-y_i+a_i(1)
\end{align}
$$

        这里(3)->(4)主要是因为 $y$ 是One-hot向量，所有元素的和为 $1$ ，即 $\sum y_k=1$ 。

        这个结果之所以重要有以下几点原因：
        - **计算高效**：在反向传播时，我们不需要重新计算复杂的指数、对数或除法导数。计算机只需要做一次减法，这极大的加快了训练速度。
        - **梯度与误差成正比（解决梯度消失问题）**：这个公式告诉我们，**梯度的物理意义就是误差**。如果预测很离谱，梯度就是 $-0.9$ ，绝对值很大 $\to$ **参数更新步子大，快速修正**。如果预测很准，梯度就是 $-0.01$ ，绝对值很小 $\to$ **微调，稳定收敛**。

## 三、在Attention中按Softmax公式计算有什么问题？
