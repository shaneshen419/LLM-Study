# Transformer架构
这部分主要介绍Transformer架构的：self-Attention机制、多头注意力(MHA、MQA、GQA)、掩码自注意力、交叉注意力(cross attention)、位置编码(Embedding)、前馈网络(FFN)、残差连接、层归一化(Batch Norm、Layer Norm、RMSNorm)各自的功能。

## 一、self-Attention机制

### 1.1. Self-Attention机制

### 1.1. Softmax的作用

## 二、多头注意力(MHA、MQA、GQA)

## 三、掩码自注意力

## 四、交叉注意力(cross attention)

## 五、位置编码(Embedding)

## 六、前馈网络(FFN)

## 七、残差连接

## 八、层归一化(Batch Norm、Layer Norm、RMSNorm)